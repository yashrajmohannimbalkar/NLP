{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMe6oWw9AotuAwxqTh7sH3m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zWJm6aVbdwg","executionInfo":{"status":"ok","timestamp":1768880505497,"user_tz":-330,"elapsed":140,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"f62a8fbb-26ce-482c-d626-ffef0f4e9e29"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}],"source":["# Imports NLTK and downloads required datasets for tokenization and lemmatization\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n"]},{"cell_type":"code","source":["# Sample text used for demonstrating NLP preprocessing techniques\n","text = \"I am Yashraj from CSE - AI.\""],"metadata":{"id":"532gcfVGdRLh","executionInfo":{"status":"ok","timestamp":1768881150154,"user_tz":-330,"elapsed":10,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Splits text into tokens using spaces as separators\n","from nltk.tokenize import WhitespaceTokenizer\n","\n","wt = WhitespaceTokenizer()\n","print(\"Whitespace Tokenization:\")\n","print(wt.tokenize(text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4HDB3MedTOl","executionInfo":{"status":"ok","timestamp":1768881165654,"user_tz":-330,"elapsed":16,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"abf649a5-27cf-47c0-cda6-2349fa89275c"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Whitespace Tokenization:\n","['I', 'am', 'Yashraj', 'from', 'CSE', '-', 'AI.']\n"]}]},{"cell_type":"code","source":["# Separates words and punctuation into individual tokens\n","from nltk.tokenize import wordpunct_tokenize\n","\n","print(\"\\nPunctuation-based Tokenization:\")\n","print(wordpunct_tokenize(text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2IUpE5MdXvl","executionInfo":{"status":"ok","timestamp":1768881180559,"user_tz":-330,"elapsed":14,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"367b4c09-48c3-4a3b-c9c8-8a54c3ba30fa"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Punctuation-based Tokenization:\n","['I', 'am', 'Yashraj', 'from', 'CSE', '-', 'AI', '.']\n"]}]},{"cell_type":"code","source":["# Tokenizes text using English grammar and punctuation rules\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","twt = TreebankWordTokenizer()\n","print(\"\\nTreebank Tokenization:\")\n","print(twt.tokenize(text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3g-Y3xXKdchV","executionInfo":{"status":"ok","timestamp":1768880674054,"user_tz":-330,"elapsed":20,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"f7d4718b-b032-4c51-8ad2-cdf2bdce1706"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Treebank Tokenization:\n","['I', 'am', 'Yashraj', 'from', 'CSE', '-', 'AI', '.']\n"]}]},{"cell_type":"code","source":["# Tokenizes social media text handling hashtags, mentions, and emojis\n","from nltk.tokenize import TweetTokenizer\n","\n","tweet_tokenizer = TweetTokenizer()\n","print(\"\\nTweet Tokenization:\")\n","print(tweet_tokenizer.tokenize(text))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_ciITk9dlPp","executionInfo":{"status":"ok","timestamp":1768880675373,"user_tz":-330,"elapsed":24,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"aa36964f-501c-4dc2-f32d-bdef26574e3c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tweet Tokenization:\n","['I', 'am', 'Yashraj', 'from', 'CSE', '-', 'AI', '.']\n"]}]},{"cell_type":"code","source":["# Combines predefined multi-word expressions into single tokens\n","from nltk.tokenize import MWETokenizer\n","\n","mwe = MWETokenizer([('machine', 'learning'), ('artificial', 'intelligence')], separator='_')\n","sentence = \"Machine learning and artificial intelligence are related fields\"\n","print(\"\\nMWE Tokenization:\")\n","print(mwe.tokenize(sentence.lower().split()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mw3rteQBdnre","executionInfo":{"status":"ok","timestamp":1768880677123,"user_tz":-330,"elapsed":12,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"b7e2085d-34a5-4d37-8d4f-77275066f043"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","MWE Tokenization:\n","['machine_learning', 'and', 'artificial_intelligence', 'are', 'related', 'fields']\n"]}]},{"cell_type":"markdown","source":["**Stemming**"],"metadata":{"id":"gItuLQ3Wdte_"}},{"cell_type":"code","source":["# Reduces words to their root form using Porter stemming algorithm\n","from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n","\n","print(\"\\nPorter Stemmer:\")\n","for word in words:\n","    print(word, \"→\", ps.stem(word))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMuOWnhYdxCY","executionInfo":{"status":"ok","timestamp":1768880678474,"user_tz":-330,"elapsed":16,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"e2ab6ed5-eaff-45ae-8cec-f997df8913c5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Porter Stemmer:\n","running → run\n","runner → runner\n","ran → ran\n","easily → easili\n","fairly → fairli\n"]}]},{"cell_type":"code","source":["# Performs improved and language-specific stemming\n","from nltk.stem import SnowballStemmer\n","\n","ss = SnowballStemmer(\"english\")\n","\n","print(\"\\nSnowball Stemmer:\")\n","for word in words:\n","    print(word, \"→\", ss.stem(word))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHfCPp7Id1YU","executionInfo":{"status":"ok","timestamp":1768880680650,"user_tz":-330,"elapsed":18,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"c768d23c-0bde-4b08-a656-28bf854ffeee"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Snowball Stemmer:\n","running → run\n","runner → runner\n","ran → ran\n","easily → easili\n","fairly → fair\n"]}]},{"cell_type":"markdown","source":["**Lemmatization (Using WordNet Lemmatizer)**"],"metadata":{"id":"ZoAQCMcsd50x"}},{"cell_type":"code","source":["# Converts words into meaningful base forms using WordNet\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","words = [\"running\", \"better\", \"cars\", \"went\"]\n","\n","print(\"\\nLemmatization:\")\n","print(\"running →\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n","print(\"better →\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n","print(\"cars →\", lemmatizer.lemmatize(\"cars\", pos=\"n\"))\n","print(\"went →\", lemmatizer.lemmatize(\"went\", pos=\"v\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6nOVRsAd9UR","executionInfo":{"status":"ok","timestamp":1768880075735,"user_tz":-330,"elapsed":4837,"user":{"displayName":"YASHRAJ NIMBALKAR","userId":"17491738209160088418"}},"outputId":"cb9fbf1e-945d-485c-9315-fd115ae9aeb7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Lemmatization:\n","running → run\n","better → good\n","cars → car\n","went → go\n"]}]}]}